{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Emotion_recognition.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "xFP0agIleILo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c502897e-4934-4512-dfcb-3b517143c557"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import keras\n",
        "\n",
        "from keras.layers import Activation, Convolution2D, Dropout, Conv2D\n",
        "from keras.layers import AveragePooling2D, BatchNormalization\n",
        "from keras.layers import GlobalAveragePooling2D\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Input\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import SeparableConv2D\n",
        "from keras import layers\n",
        "from keras.regularizers import l2\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "ExOrW6AO9KJM",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "625cfd59-39ed-44dd-d894-090f31de090f"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-80875be3-0a74-4f95-848a-9039000a6a4b\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-80875be3-0a74-4f95-848a-9039000a6a4b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle (1).json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"cleanpegasus\",\"key\":\"e9ef704ab5501fb855464d9696c823c7\"}'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "0wDiFQCxehwD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -q kaggle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-NPQuemd_G6z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pph0t-yU_JZ3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cp kaggle.json ~/.kaggle/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pvyu5GZc_Hog",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "lva2GnO8_U6_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "ec6c34c8-e8d0-411e-d0da-9abd459b0401"
      },
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c challenges-in-representation-learning-facial-expression-recognition-challenge"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Downloading example_submission.csv to /content\n",
            "  0% 0.00/7.01k [00:00<?, ?B/s]\n",
            "100% 7.01k/7.01k [00:00<00:00, 14.0MB/s]\n",
            "Downloading fer2013.tar.gz to /content\n",
            " 84% 77.0M/92.0M [00:01<00:00, 50.9MB/s]\n",
            "100% 92.0M/92.0M [00:01<00:00, 79.2MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "R0lAS63L_VTF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "5c34896e-ef45-4710-ce57-85b67b632a33"
      },
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " data                     \u001b[0m\u001b[01;34mfer2013\u001b[0m/        'kaggle (1).json'   \u001b[01;34msample_data\u001b[0m/\n",
            " example_submission.csv   fer2013.tar.gz   kaggle.json        train_data.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SXz2sF3jAs8i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!tar -xzf fer2013.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C_lSxcyOBG4q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "48f6a4ea-9637-48ad-d27b-e0ad3645c018"
      },
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data                    \u001b[0m\u001b[01;34mfer2013\u001b[0m/        kaggle.json   train_data.csv\n",
            "example_submission.csv  fer2013.tar.gz  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QwbfwjJbeIMD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('fer2013/fer2013.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "etoab7k8eIMQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "image = np.asarray(df['pixels'])\n",
        "y_train = np.asarray(df['emotion'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-2ajD-eLeIMY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "metadata": {
        "id": "GjLpGxhjeIMb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train = []\n",
        "for i in range(np.shape(image)[0]):\n",
        "    im = image[i]\n",
        "    im = im.split(' ')\n",
        "    im = np.asarray(im)\n",
        "    X_train.append(im.reshape(48, 48))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5Rq9CDQJeIMg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2a2a3f8a-3294-497c-e9f0-58ecb5affade"
      },
      "cell_type": "code",
      "source": [
        "X_train = np.asarray(X_train, dtype = float)\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(35887, 48, 48)\n",
            "(35887,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "H3Gpfx2UeIMq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "outputId": "f2db8ef2-bcc8-4dc1-c3f4-6019a9bc4510"
      },
      "cell_type": "code",
      "source": [
        "x = random.randint(0, np.shape(image)[0])\n",
        "q = np.asarray(X_train[x], dtype=float)\n",
        "print(y_train[x])\n",
        "plt.imshow(q, cmap = plt.get_cmap(\"gray\"))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f8beb92a160>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFLCAYAAABft66eAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "nBV0g09aeIMv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_classes = 7\n",
        "y_train = to_categorical(y_train, 7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IYpRZgNEeIMy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "92f6694b-7286-4fda-b6bb-d8344da56b0f"
      },
      "cell_type": "code",
      "source": [
        "print(X_train.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(35887, 48, 48)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SZqjWvwieIM3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train = X_train/255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1ymUALsKeIM6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train = X_train.reshape(35887, 48, 48, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HhHCpWeVeIM8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_generator = ImageDataGenerator(\n",
        "                        featurewise_center=False,\n",
        "                        featurewise_std_normalization=False,\n",
        "                        rotation_range=10,\n",
        "                        width_shift_range=0.1,\n",
        "                        height_shift_range=0.1,\n",
        "                        zoom_range=.1,\n",
        "                        horizontal_flip=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tADqWAQveIM_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def neural_model(input_shape, num_classes, l2_regularization=0.01):\n",
        "    regularization = l2(l2_regularization)\n",
        "\n",
        "    # base\n",
        "    img_input = Input(input_shape)\n",
        "    x = Conv2D(8, (3, 3), strides=(1, 1), kernel_regularizer=regularization,\n",
        "               use_bias=False)(img_input)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(8, (3, 3), strides=(1, 1), kernel_regularizer=regularization,\n",
        "               use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    # module 1\n",
        "    residual = Conv2D(16, (1, 1), strides=(2, 2),\n",
        "                      padding='same', use_bias=False)(x)\n",
        "    residual = BatchNormalization()(residual)\n",
        "\n",
        "    x = SeparableConv2D(16, (3, 3), padding='same',\n",
        "                        kernel_regularizer=regularization,\n",
        "                        use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = SeparableConv2D(16, (3, 3), padding='same',\n",
        "                        kernel_regularizer=regularization,\n",
        "                        use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
        "    x = layers.add([x, residual])\n",
        "\n",
        "    # module 2\n",
        "    residual = Conv2D(32, (1, 1), strides=(2, 2),\n",
        "                      padding='same', use_bias=False)(x)\n",
        "    residual = BatchNormalization()(residual)\n",
        "\n",
        "    x = SeparableConv2D(32, (3, 3), padding='same',\n",
        "                        kernel_regularizer=regularization,\n",
        "                        use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = SeparableConv2D(32, (3, 3), padding='same',\n",
        "                        kernel_regularizer=regularization,\n",
        "                        use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
        "    x = layers.add([x, residual])\n",
        "\n",
        "    # module 3\n",
        "    residual = Conv2D(64, (1, 1), strides=(2, 2),\n",
        "                      padding='same', use_bias=False)(x)\n",
        "    residual = BatchNormalization()(residual)\n",
        "\n",
        "    x = SeparableConv2D(64, (3, 3), padding='same',\n",
        "                        kernel_regularizer=regularization,\n",
        "                        use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = SeparableConv2D(64, (3, 3), padding='same',\n",
        "                        kernel_regularizer=regularization,\n",
        "                        use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
        "    x = layers.add([x, residual])\n",
        "\n",
        "    # module 4\n",
        "    residual = Conv2D(128, (1, 1), strides=(2, 2),\n",
        "                      padding='same', use_bias=False)(x)\n",
        "    residual = BatchNormalization()(residual)\n",
        "\n",
        "    x = SeparableConv2D(128, (3, 3), padding='same',\n",
        "                        kernel_regularizer=regularization,\n",
        "                        use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = SeparableConv2D(128, (3, 3), padding='same',\n",
        "                        kernel_regularizer=regularization,\n",
        "                        use_bias=False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
        "    x = layers.add([x, residual])\n",
        "\n",
        "    x = Conv2D(num_classes, (3, 3),\n",
        "               # kernel_regularizer=regularization,\n",
        "               padding='same')(x)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    output = Activation('softmax', name='predictions')(x)\n",
        "\n",
        "    model = Model(img_input, output)\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sMl1CI6neIND",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1805
        },
        "outputId": "f42b4fc5-e2b1-4b6d-95f2-0468598bdad3"
      },
      "cell_type": "code",
      "source": [
        "input_shape = (48, 48, 1)\n",
        "model = neural_model(input_shape, num_classes)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 48, 48, 1)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 46, 46, 8)    72          input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 46, 46, 8)    32          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 46, 46, 8)    0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 44, 44, 8)    576         activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 44, 44, 8)    32          conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 44, 44, 8)    0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_1 (SeparableCo (None, 44, 44, 16)   200         activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 44, 44, 16)   64          separable_conv2d_1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 44, 44, 16)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_2 (SeparableCo (None, 44, 44, 16)   400         activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 44, 44, 16)   64          separable_conv2d_2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 22, 22, 16)   128         activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 22, 22, 16)   0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 22, 22, 16)   64          conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 22, 22, 16)   0           max_pooling2d_1[0][0]            \n",
            "                                                                 batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_3 (SeparableCo (None, 22, 22, 32)   656         add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 22, 22, 32)   128         separable_conv2d_3[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 22, 22, 32)   0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_4 (SeparableCo (None, 22, 22, 32)   1312        activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 22, 22, 32)   128         separable_conv2d_4[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 11, 11, 32)   512         add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 11, 11, 32)   0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 11, 11, 32)   128         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 11, 11, 32)   0           max_pooling2d_2[0][0]            \n",
            "                                                                 batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_5 (SeparableCo (None, 11, 11, 64)   2336        add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 11, 11, 64)   256         separable_conv2d_5[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 11, 11, 64)   0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_6 (SeparableCo (None, 11, 11, 64)   4672        activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 11, 11, 64)   256         separable_conv2d_6[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 6, 6, 64)     2048        add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 6, 6, 64)     0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 6, 6, 64)     256         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 6, 6, 64)     0           max_pooling2d_3[0][0]            \n",
            "                                                                 batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_7 (SeparableCo (None, 6, 6, 128)    8768        add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 6, 6, 128)    512         separable_conv2d_7[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 6, 6, 128)    0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "separable_conv2d_8 (SeparableCo (None, 6, 6, 128)    17536       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 6, 6, 128)    512         separable_conv2d_8[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 3, 3, 128)    8192        add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 3, 3, 128)    0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 3, 3, 128)    512         conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 3, 3, 128)    0           max_pooling2d_4[0][0]            \n",
            "                                                                 batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 3, 3, 7)      8071        add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_1 (Glo (None, 7)            0           conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "predictions (Activation)        (None, 7)            0           global_average_pooling2d_1[0][0] \n",
            "==================================================================================================\n",
            "Total params: 58,423\n",
            "Trainable params: 56,951\n",
            "Non-trainable params: 1,472\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "X5QBWlyOeINH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w2jy0olaeINK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "7c437206-2dcb-41c8-aa72-8018332a1200"
      },
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(28709, 48, 48, 1)\n",
            "(28709, 7)\n",
            "(7178, 48, 48, 1)\n",
            "(7178, 7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hiLrXI0YeINO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4382
        },
        "outputId": "10f231b6-1c69-4a30-f02b-ee3115b03b91"
      },
      "cell_type": "code",
      "source": [
        "model.fit_generator(data_generator.flow(X_train, y_train, 32),\n",
        "                   steps_per_epoch = len(X_train)/32,\n",
        "                   epochs = 500,\n",
        "                   verbose = 1,\n",
        "                   validation_data = (X_val, y_val))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "898/897 [==============================] - 49s 55ms/step - loss: 1.8009 - acc: 0.3125 - val_loss: 1.6176 - val_acc: 0.3963\n",
            "Epoch 2/500\n",
            "898/897 [==============================] - 47s 52ms/step - loss: 1.5378 - acc: 0.4225 - val_loss: 1.5578 - val_acc: 0.4246\n",
            "Epoch 3/500\n",
            "898/897 [==============================] - 46s 52ms/step - loss: 1.4122 - acc: 0.4713 - val_loss: 1.3889 - val_acc: 0.4812\n",
            "Epoch 4/500\n",
            "898/897 [==============================] - 48s 53ms/step - loss: 1.3422 - acc: 0.4964 - val_loss: 1.3088 - val_acc: 0.5045\n",
            "Epoch 5/500\n",
            "898/897 [==============================] - 48s 53ms/step - loss: 1.2952 - acc: 0.5152 - val_loss: 1.3784 - val_acc: 0.4868\n",
            "Epoch 6/500\n",
            "898/897 [==============================] - 46s 52ms/step - loss: 1.2594 - acc: 0.5292 - val_loss: 1.3137 - val_acc: 0.5091\n",
            "Epoch 7/500\n",
            "898/897 [==============================] - 47s 52ms/step - loss: 1.2332 - acc: 0.5355 - val_loss: 1.2517 - val_acc: 0.5332\n",
            "Epoch 8/500\n",
            "898/897 [==============================] - 47s 52ms/step - loss: 1.2136 - acc: 0.5465 - val_loss: 1.2944 - val_acc: 0.5273\n",
            "Epoch 9/500\n",
            "898/897 [==============================] - 47s 52ms/step - loss: 1.1922 - acc: 0.5560 - val_loss: 1.2912 - val_acc: 0.5091\n",
            "Epoch 10/500\n",
            "898/897 [==============================] - 48s 54ms/step - loss: 1.1792 - acc: 0.5574 - val_loss: 1.2890 - val_acc: 0.5095\n",
            "Epoch 11/500\n",
            "898/897 [==============================] - 49s 55ms/step - loss: 1.1613 - acc: 0.5653 - val_loss: 1.1383 - val_acc: 0.5745\n",
            "Epoch 12/500\n",
            "898/897 [==============================] - 47s 53ms/step - loss: 1.1438 - acc: 0.5715 - val_loss: 1.1194 - val_acc: 0.5876\n",
            "Epoch 13/500\n",
            "898/897 [==============================] - 47s 52ms/step - loss: 1.1324 - acc: 0.5771 - val_loss: 1.1241 - val_acc: 0.5864\n",
            "Epoch 14/500\n",
            "898/897 [==============================] - 47s 52ms/step - loss: 1.1232 - acc: 0.5792 - val_loss: 1.1296 - val_acc: 0.5841\n",
            "Epoch 15/500\n",
            "898/897 [==============================] - 46s 52ms/step - loss: 1.1159 - acc: 0.5815 - val_loss: 1.1901 - val_acc: 0.5756\n",
            "Epoch 16/500\n",
            "898/897 [==============================] - 46s 52ms/step - loss: 1.0966 - acc: 0.5884 - val_loss: 1.1696 - val_acc: 0.5606\n",
            "Epoch 17/500\n",
            "898/897 [==============================] - 48s 53ms/step - loss: 1.0947 - acc: 0.5933 - val_loss: 1.1395 - val_acc: 0.5698\n",
            "Epoch 18/500\n",
            "898/897 [==============================] - 47s 52ms/step - loss: 1.0815 - acc: 0.5972 - val_loss: 1.1262 - val_acc: 0.5627\n",
            "Epoch 19/500\n",
            "898/897 [==============================] - 46s 51ms/step - loss: 1.0750 - acc: 0.5989 - val_loss: 1.1145 - val_acc: 0.5879\n",
            "Epoch 20/500\n",
            "898/897 [==============================] - 46s 52ms/step - loss: 1.0666 - acc: 0.6006 - val_loss: 1.0936 - val_acc: 0.5819\n",
            "Epoch 21/500\n",
            "898/897 [==============================] - 47s 52ms/step - loss: 1.0607 - acc: 0.6026 - val_loss: 1.1082 - val_acc: 0.5925\n",
            "Epoch 22/500\n",
            "898/897 [==============================] - 47s 52ms/step - loss: 1.0598 - acc: 0.6006 - val_loss: 1.1230 - val_acc: 0.5848\n",
            "Epoch 23/500\n",
            "898/897 [==============================] - 47s 52ms/step - loss: 1.0480 - acc: 0.6081 - val_loss: 1.0808 - val_acc: 0.5981\n",
            "Epoch 24/500\n",
            "898/897 [==============================] - 48s 53ms/step - loss: 1.0472 - acc: 0.6098 - val_loss: 1.0535 - val_acc: 0.6046\n",
            "Epoch 25/500\n",
            "898/897 [==============================] - 47s 53ms/step - loss: 1.0413 - acc: 0.6124 - val_loss: 1.1540 - val_acc: 0.5690\n",
            "Epoch 26/500\n",
            "898/897 [==============================] - 46s 52ms/step - loss: 1.0366 - acc: 0.6108 - val_loss: 1.0693 - val_acc: 0.5991\n",
            "Epoch 27/500\n",
            "898/897 [==============================] - 46s 52ms/step - loss: 1.0317 - acc: 0.6145 - val_loss: 1.0396 - val_acc: 0.6060\n",
            "Epoch 28/500\n",
            "898/897 [==============================] - 46s 52ms/step - loss: 1.0235 - acc: 0.6191 - val_loss: 1.1133 - val_acc: 0.5899\n",
            "Epoch 29/500\n",
            "898/897 [==============================] - 47s 52ms/step - loss: 1.0191 - acc: 0.6198 - val_loss: 1.0353 - val_acc: 0.6024\n",
            "Epoch 30/500\n",
            "898/897 [==============================] - 48s 53ms/step - loss: 1.0171 - acc: 0.6196 - val_loss: 1.1050 - val_acc: 0.5812\n",
            "Epoch 31/500\n",
            "898/897 [==============================] - 47s 53ms/step - loss: 1.0136 - acc: 0.6219 - val_loss: 1.0449 - val_acc: 0.6078\n",
            "Epoch 32/500\n",
            "898/897 [==============================] - 46s 51ms/step - loss: 1.0099 - acc: 0.6195 - val_loss: 1.0659 - val_acc: 0.5925\n",
            "Epoch 33/500\n",
            "898/897 [==============================] - 46s 51ms/step - loss: 1.0057 - acc: 0.6245 - val_loss: 1.0592 - val_acc: 0.6041\n",
            "Epoch 34/500\n",
            "898/897 [==============================] - 46s 51ms/step - loss: 1.0051 - acc: 0.6256 - val_loss: 1.0598 - val_acc: 0.5956\n",
            "Epoch 35/500\n",
            "898/897 [==============================] - 46s 51ms/step - loss: 0.9945 - acc: 0.6279 - val_loss: 1.0743 - val_acc: 0.5979\n",
            "Epoch 36/500\n",
            "898/897 [==============================] - 46s 51ms/step - loss: 0.9950 - acc: 0.6268 - val_loss: 1.0261 - val_acc: 0.6208\n",
            "Epoch 37/500\n",
            "898/897 [==============================] - 47s 53ms/step - loss: 0.9857 - acc: 0.6306 - val_loss: 1.0998 - val_acc: 0.5876\n",
            "Epoch 38/500\n",
            "898/897 [==============================] - 46s 52ms/step - loss: 0.9898 - acc: 0.6313 - val_loss: 1.0218 - val_acc: 0.6201\n",
            "Epoch 39/500\n",
            "898/897 [==============================] - 46s 51ms/step - loss: 0.9885 - acc: 0.6292 - val_loss: 1.0232 - val_acc: 0.6134\n",
            "Epoch 40/500\n",
            "898/897 [==============================] - 46s 51ms/step - loss: 0.9822 - acc: 0.6348 - val_loss: 1.0158 - val_acc: 0.6151\n",
            "Epoch 41/500\n",
            "898/897 [==============================] - 46s 51ms/step - loss: 0.9782 - acc: 0.6387 - val_loss: 1.0328 - val_acc: 0.6140\n",
            "Epoch 42/500\n",
            "898/897 [==============================] - 46s 51ms/step - loss: 0.9791 - acc: 0.6334 - val_loss: 1.0634 - val_acc: 0.6088\n",
            "Epoch 43/500\n",
            "898/897 [==============================] - 46s 51ms/step - loss: 0.9744 - acc: 0.6379 - val_loss: 1.0241 - val_acc: 0.6183\n",
            "Epoch 44/500\n",
            "898/897 [==============================] - 48s 54ms/step - loss: 0.9735 - acc: 0.6389 - val_loss: 1.0212 - val_acc: 0.6138\n",
            "Epoch 45/500\n",
            "898/897 [==============================] - 46s 51ms/step - loss: 0.9687 - acc: 0.6400 - val_loss: 1.0158 - val_acc: 0.6172\n",
            "Epoch 46/500\n",
            "898/897 [==============================] - 46s 51ms/step - loss: 0.9653 - acc: 0.6391 - val_loss: 1.0335 - val_acc: 0.6151\n",
            "Epoch 47/500\n",
            "898/897 [==============================] - 46s 52ms/step - loss: 0.9642 - acc: 0.6428 - val_loss: 1.1086 - val_acc: 0.5940\n",
            "Epoch 48/500\n",
            "898/897 [==============================] - 46s 51ms/step - loss: 0.9644 - acc: 0.6391 - val_loss: 1.0157 - val_acc: 0.6250\n",
            "Epoch 49/500\n",
            "898/897 [==============================] - 46s 51ms/step - loss: 0.9578 - acc: 0.6450 - val_loss: 1.0148 - val_acc: 0.6243\n",
            "Epoch 50/500\n",
            "898/897 [==============================] - 46s 51ms/step - loss: 0.9629 - acc: 0.6403 - val_loss: 1.0871 - val_acc: 0.5993\n",
            "Epoch 51/500\n",
            "898/897 [==============================] - 48s 54ms/step - loss: 0.9542 - acc: 0.6431 - val_loss: 1.0083 - val_acc: 0.6308\n",
            "Epoch 52/500\n",
            "898/897 [==============================] - 46s 51ms/step - loss: 0.9503 - acc: 0.6453 - val_loss: 1.0387 - val_acc: 0.6191\n",
            "Epoch 53/500\n",
            "898/897 [==============================] - 46s 51ms/step - loss: 0.9490 - acc: 0.6465 - val_loss: 1.0596 - val_acc: 0.6144\n",
            "Epoch 54/500\n",
            "898/897 [==============================] - 46s 51ms/step - loss: 0.9442 - acc: 0.6494 - val_loss: 1.0282 - val_acc: 0.6297\n",
            "Epoch 55/500\n",
            "898/897 [==============================] - 46s 51ms/step - loss: 0.9487 - acc: 0.6495 - val_loss: 1.0012 - val_acc: 0.6385\n",
            "Epoch 56/500\n",
            "898/897 [==============================] - 46s 51ms/step - loss: 0.9466 - acc: 0.6479 - val_loss: 1.0813 - val_acc: 0.6088\n",
            "Epoch 57/500\n",
            "898/897 [==============================] - 48s 53ms/step - loss: 0.9429 - acc: 0.6493 - val_loss: 1.0410 - val_acc: 0.6106\n",
            "Epoch 58/500\n",
            "898/897 [==============================] - 47s 52ms/step - loss: 0.9390 - acc: 0.6523 - val_loss: 1.0141 - val_acc: 0.6230\n",
            "Epoch 59/500\n",
            "898/897 [==============================] - 46s 51ms/step - loss: 0.9357 - acc: 0.6524 - val_loss: 1.0394 - val_acc: 0.6176\n",
            "Epoch 60/500\n",
            "898/897 [==============================] - 46s 51ms/step - loss: 0.9353 - acc: 0.6513 - val_loss: 1.0304 - val_acc: 0.6239\n",
            "Epoch 61/500\n",
            "898/897 [==============================] - 46s 51ms/step - loss: 0.9317 - acc: 0.6554 - val_loss: 1.0223 - val_acc: 0.6186\n",
            "Epoch 62/500\n",
            "898/897 [==============================] - 46s 51ms/step - loss: 0.9325 - acc: 0.6527 - val_loss: 1.0370 - val_acc: 0.6094\n",
            "Epoch 63/500\n",
            "898/897 [==============================] - 46s 51ms/step - loss: 0.9294 - acc: 0.6542 - val_loss: 1.0095 - val_acc: 0.6343\n",
            "Epoch 64/500\n",
            "898/897 [==============================] - 48s 54ms/step - loss: 0.9313 - acc: 0.6520 - val_loss: 1.0105 - val_acc: 0.6245\n",
            "Epoch 65/500\n",
            "898/897 [==============================] - 46s 51ms/step - loss: 0.9262 - acc: 0.6557 - val_loss: 1.0737 - val_acc: 0.6066\n",
            "Epoch 66/500\n",
            "898/897 [==============================] - 46s 52ms/step - loss: 0.9237 - acc: 0.6557 - val_loss: 1.0602 - val_acc: 0.6169\n",
            "Epoch 67/500\n",
            "898/897 [==============================] - 46s 51ms/step - loss: 0.9221 - acc: 0.6574 - val_loss: 1.0859 - val_acc: 0.6145\n",
            "Epoch 68/500\n",
            "898/897 [==============================] - 46s 51ms/step - loss: 0.9248 - acc: 0.6551 - val_loss: 1.0100 - val_acc: 0.6287\n",
            "Epoch 69/500\n",
            "898/897 [==============================] - 46s 51ms/step - loss: 0.9207 - acc: 0.6562 - val_loss: 1.0507 - val_acc: 0.6073\n",
            "Epoch 70/500\n",
            "898/897 [==============================] - 46s 51ms/step - loss: 0.9224 - acc: 0.6550 - val_loss: 1.0330 - val_acc: 0.6206\n",
            "Epoch 71/500\n",
            "898/897 [==============================] - 47s 53ms/step - loss: 0.9191 - acc: 0.6574 - val_loss: 0.9912 - val_acc: 0.6330\n",
            "Epoch 72/500\n",
            "898/897 [==============================] - 46s 51ms/step - loss: 0.9156 - acc: 0.6614 - val_loss: 0.9964 - val_acc: 0.6325\n",
            "Epoch 73/500\n",
            "898/897 [==============================] - 46s 51ms/step - loss: 0.9120 - acc: 0.6614 - val_loss: 1.0566 - val_acc: 0.6173\n",
            "Epoch 74/500\n",
            "898/897 [==============================] - 46s 51ms/step - loss: 0.9145 - acc: 0.6617 - val_loss: 1.0148 - val_acc: 0.6259\n",
            "Epoch 75/500\n",
            "898/897 [==============================] - 46s 51ms/step - loss: 0.9090 - acc: 0.6617 - val_loss: 1.0000 - val_acc: 0.6323\n",
            "Epoch 76/500\n",
            "898/897 [==============================] - 46s 51ms/step - loss: 0.9069 - acc: 0.6625 - val_loss: 1.0064 - val_acc: 0.6339\n",
            "Epoch 77/500\n",
            "898/897 [==============================] - 48s 53ms/step - loss: 0.9177 - acc: 0.6599 - val_loss: 1.0772 - val_acc: 0.6135\n",
            "Epoch 78/500\n",
            "898/897 [==============================] - 46s 51ms/step - loss: 0.9038 - acc: 0.6633 - val_loss: 1.0447 - val_acc: 0.6237\n",
            "Epoch 79/500\n",
            "898/897 [==============================] - 46s 51ms/step - loss: 0.9074 - acc: 0.6647 - val_loss: 1.0486 - val_acc: 0.6145\n",
            "Epoch 80/500\n",
            "898/897 [==============================] - 46s 51ms/step - loss: 0.9044 - acc: 0.6636 - val_loss: 1.0034 - val_acc: 0.6290\n",
            "Epoch 81/500\n",
            "898/897 [==============================] - 46s 51ms/step - loss: 0.9031 - acc: 0.6668 - val_loss: 1.0096 - val_acc: 0.6318\n",
            "Epoch 82/500\n",
            "898/897 [==============================] - 46s 51ms/step - loss: 0.8980 - acc: 0.6639 - val_loss: 1.0293 - val_acc: 0.6233\n",
            "Epoch 83/500\n",
            "898/897 [==============================] - 46s 51ms/step - loss: 0.9032 - acc: 0.6642 - val_loss: 1.0410 - val_acc: 0.6215\n",
            "Epoch 84/500\n",
            "898/897 [==============================] - 48s 54ms/step - loss: 0.9001 - acc: 0.6665 - val_loss: 1.0218 - val_acc: 0.6187\n",
            "Epoch 85/500\n",
            "898/897 [==============================] - 46s 51ms/step - loss: 0.9026 - acc: 0.6627 - val_loss: 1.0159 - val_acc: 0.6314\n",
            "Epoch 86/500\n",
            "898/897 [==============================] - 46s 51ms/step - loss: 0.8971 - acc: 0.6662 - val_loss: 1.0040 - val_acc: 0.6304\n",
            "Epoch 87/500\n",
            "898/897 [==============================] - 46s 51ms/step - loss: 0.8933 - acc: 0.6701 - val_loss: 1.0265 - val_acc: 0.6236\n",
            "Epoch 88/500\n",
            "898/897 [==============================] - 46s 51ms/step - loss: 0.8954 - acc: 0.6668 - val_loss: 1.0054 - val_acc: 0.6388\n",
            "Epoch 89/500\n",
            "898/897 [==============================] - 46s 51ms/step - loss: 0.8897 - acc: 0.6707 - val_loss: 1.0027 - val_acc: 0.6304\n",
            "Epoch 90/500\n",
            "898/897 [==============================] - 46s 51ms/step - loss: 0.8949 - acc: 0.6672 - val_loss: 1.0236 - val_acc: 0.6337\n",
            "Epoch 91/500\n",
            "898/897 [==============================] - 48s 53ms/step - loss: 0.8971 - acc: 0.6663 - val_loss: 1.0032 - val_acc: 0.6344\n",
            "Epoch 92/500\n",
            "898/897 [==============================] - 46s 51ms/step - loss: 0.8901 - acc: 0.6694 - val_loss: 1.0351 - val_acc: 0.6194\n",
            "Epoch 93/500\n",
            "898/897 [==============================] - 46s 51ms/step - loss: 0.8865 - acc: 0.6718 - val_loss: 1.0406 - val_acc: 0.6199\n",
            "Epoch 94/500\n",
            "898/897 [==============================] - 46s 51ms/step - loss: 0.8851 - acc: 0.6718 - val_loss: 1.1014 - val_acc: 0.5945\n",
            "Epoch 95/500\n",
            "898/897 [==============================] - 46s 51ms/step - loss: 0.8876 - acc: 0.6690 - val_loss: 1.0002 - val_acc: 0.6319\n",
            "Epoch 96/500\n",
            "898/897 [==============================] - 46s 51ms/step - loss: 0.8832 - acc: 0.6727 - val_loss: 1.0118 - val_acc: 0.6339\n",
            "Epoch 97/500\n",
            "898/897 [==============================] - 47s 52ms/step - loss: 0.8870 - acc: 0.6709 - val_loss: 1.0451 - val_acc: 0.6184\n",
            "Epoch 98/500\n",
            "359/897 [===========>..................] - ETA: 27s - loss: 0.8740 - acc: 0.6762"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-dd1053e2f69e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                    \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                    \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                    validation_data = (X_val, y_val))\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "dAKVCpjIeINU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.save('recog.h5')\n",
        "from google.colab import files\n",
        "files.download('recog.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z4JvWYCU2c1r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}